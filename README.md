# Advanced NLP Project

## Introduction
This is the project on `A Decomposable Attention Model for Natural Language Inference`, by Ahish Deshpande (2018102022) and Jai Bardhan (2018113008).

## Run Instructions

* #### Creating the Enviroment
    * Create a virtual environment using `virtualenv` or a new conda environment (recommended) with `conda create -n nli_project python=3.7`
    * Use the given `requirements.txt` file to install all dependencies with `pip install -r requirements.txt`
* #### Downloading the Data
    * Download the source code, and extract to a desired directory
    * Download the SNLI dataset from [here](https://nlp.stanford.edu/projects/snli/snli_1.0.zip) and the GloVe word embeddings from [here](https://nlp.stanford.edu/data/glove.6B.zip) 
    * Extract both the downloaded files using the `unzip` command

* #### Preprocessing the Data
    * Run the `process-snli.py` script as follows:
        ```
            usage: process-snli.py [-h] [--data_folder DATA_FOLDER]
                               [--out_folder OUT_FOLDER]
        
        optional arguments:
          -h, --help            show this help message and exit
          --data_folder DATA_FOLDER
                                location of folder with the snli files (default: None)
          --out_folder OUT_FOLDER
                                location of the output folder (default: None)
        ```
    * Then, run `preprocess.sh`, after modifying the paths to the files generated by the above script, to get the final preprocessed data. It can also be run manually as follows:
        ```
        usage: preprocess-entail.py [-h] [--vocabsize VOCABSIZE] [--srcfile SRCFILE]
                                    [--targetfile TARGETFILE] [--labelfile LABELFILE]
                                    [--srcvalfile SRCVALFILE]
                                    [--targetvalfile TARGETVALFILE]
                                    [--labelvalfile LABELVALFILE]
                                    [--srctestfile SRCTESTFILE]
                                    [--targettestfile TARGETTESTFILE]
                                    [--labeltestfile LABELTESTFILE]
                                    [--batchsize BATCHSIZE] [--seqlength SEQLENGTH]
                                    [--outputfile OUTPUTFILE] [--vocabfile VOCABFILE]
                                    [--shuffle SHUFFLE] [--glove GLOVE]
        
        Create the data for sentence pair classification
        
        optional arguments:
          -h, --help            show this help message and exit
          --vocabsize VOCABSIZE
                                Size of source vocabulary, constructed by taking the
                                top X most frequent words. Rest are replaced with
                                special UNK tokens. (default: 50000)
          --srcfile SRCFILE     Path to sent1 training data. (default:
                                data/entail/src-train.txt)
          --targetfile TARGETFILE
                                Path to sent2 training data. (default:
                                data/entail/targ-train.txt)
          --labelfile LABELFILE
                                Path to label data, where each line represents a
                                single label for the sentence pair. (default:
                                data/entail/label-train.txt)
          --srcvalfile SRCVALFILE
                                Path to sent1 validation data. (default:
                                data/entail/src-dev.txt)
          --targetvalfile TARGETVALFILE
                                Path to sent2 validation data. (default:
                                data/entail/targ-dev.txt)
          --labelvalfile LABELVALFILE
                                Path to label validation data. (default:
                                data/entail/label-dev.txt)
          --srctestfile SRCTESTFILE
                                Path to sent1 test data. (default: data/entail/src-
                                test.txt)
          --targettestfile TARGETTESTFILE
                                Path to sent2 test data. (default: data/entail/targ-
                                test.txt)
          --labeltestfile LABELTESTFILE
                                Path to label test data. (default: data/entail/label-
                                test.txt)
          --batchsize BATCHSIZE
                                Size of each minibatch. (default: 32)
          --seqlength SEQLENGTH
                                Maximum sequence length. Sequences longer than this
                                are dropped. (default: 100)
          --outputfile OUTPUTFILE
                                Prefix of the output file names. (default:
                                data/entail)
          --vocabfile VOCABFILE
                                If working with a preset vocab, then including this
                                will ignore vocabsize and use thevocab provided here.
                                (default: )
          --shuffle SHUFFLE     If = 1, shuffle sentences before sorting (based on
                                source length). (default: 1)
          --glove GLOVE
        ```
    * Run the `get_pretrain_vecs.py` script as follows:
        ```
        usage: get_pretrain_vecs.py [-h] [--dictionary DICTIONARY] [--glove GLOVE]
                                    [--outputfile OUTPUTFILE]
        
        optional arguments:
          -h, --help            show this help message and exit
          --dictionary DICTIONARY
                                *.dict file
          --glove GLOVE         pretrained word vectors
          --outputfile OUTPUTFILE
                                output hdf5 file
        ```
    * Rename the generated `glove.hdf5` file to `w2v.hdf5`
    * Store all the generated files in a folder called `data` at the root of the code directory
* #### Verifying the Directory Structure
    * At the end of the preprocessing, the tree structure of the code directory should look as follows:
        ```
        .
        |-- data
        |   |-- entail.label.dict
        |   |-- entail-test.hdf5
        |   |-- entail-train.hdf5
        |   |-- entail-val.hdf5
        |   |-- entail.word.dict
        |   |-- __init__.py
        |   `-- w2v.hdf5
        |-- logs
        |-- model
        |-- preprocess
        |   |-- get_pretrain_vecs.py
        |   |-- preprocess-entail.py
        |   |-- preprocess.sh
        |   `-- process-snli.py
        |-- README.md
        |-- requirements.txt
        |-- src
        |   |-- data_models.py
        |   |-- __init__.py
        |   |-- lightning_modules.py
        |   |-- logs
        |   |-- models.py
        |-- test.py
        |-- train.py
        ```
* #### Training the Model
    * Now, we can start training using the `train.py` script. It can simply be run with the default settings as `python train.py` or the arguments can be changed as:
        ```
        usage: train.py [-h] [--train_file TRAIN_FILE] [--val_file VAL_FILE]
                        [--test_file TEST_FILE] [--max_length MAX_LENGTH]
                        [--w2v_file W2V_FILE] [--embedding_size EMBEDDING_SIZE]
                        [--hidden_size HIDDEN_SIZE] [--intra_sent_atten]
                        [--epoch EPOCH] [--gpus GPUS] [--val_interval VAL_INTERVAL]
                        [--num_workers NUM_WORKERS] [--optimizer {adam,adagrad}]
                        [--lr LR] [--max_grad_norm MAX_GRAD_NORM]
                        [--param_init PARAM_INIT] [--weight_decay WEIGHT_DECAY]
                        [--log_dir LOG_DIR] [--model_path MODEL_PATH] [--use_wandb]
        
        optional arguments:
          -h, --help            show this help message and exit
          --train_file TRAIN_FILE
                                training data file (hdf5)
          --val_file VAL_FILE   validation data file (hdf5)
          --test_file TEST_FILE
                                test data file (hdf5)
          --max_length MAX_LENGTH
                                maximum length of training sentences. -1 means no max
                                length
          --w2v_file W2V_FILE   pretrained word vectors file (hdf5)
          --embedding_size EMBEDDING_SIZE
                                word embedding size
          --hidden_size HIDDEN_SIZE
                                hidden layer size
          --intra_sent_atten    whether to use intra sentence attention
          --epoch EPOCH         number of training epochs
          --gpus GPUS           number of gpus to train on. -1 for all gpus
          --val_interval VAL_INTERVAL
                                interval for checking the validation dataset
          --num_workers NUM_WORKERS
                                number of workers in the dataloader
          --optimizer {adam,adagrad}
          --lr LR               learning rate
          --max_grad_norm MAX_GRAD_NORM
                                if the norm of the gradient vector exceeds this
                                renormalize it to have the norm equal to the
                                max_grad_norm
          --param_init PARAM_INIT
                                parameter initialization gaussian
          --weight_decay WEIGHT_DECAY
                                l2 regularization
          --log_dir LOG_DIR     log file directory
          --model_path MODEL_PATH
                                path of the model (w/o suffix)
          --use_wandb           whether to use wandb for logging
        ```
* #### Testing the Model
    * Now, we can test the model using the `test.py` script, which can be run with the following arguments
        ```
        usage: test.py [-h] [--train_file TRAIN_FILE] [--val_file VAL_FILE]
                       [--test_file TEST_FILE] [--max_length MAX_LENGTH]
                       [--w2v_file W2V_FILE] [--embedding_size EMBEDDING_SIZE]
                       [--hidden_size HIDDEN_SIZE] [--intra_sent_atten]
                       [--epoch EPOCH] [--gpus GPUS] [--val_interval VAL_INTERVAL]
                       [--num_workers NUM_WORKERS] [--optimizer {adam,adagrad}]
                       [--lr LR] [--max_grad_norm MAX_GRAD_NORM]
                       [--param_init PARAM_INIT] [--weight_decay WEIGHT_DECAY]
                       [--log_dir LOG_DIR] [--model_path MODEL_PATH] [--use_wandb]
        
        optional arguments:
          -h, --help            show this help message and exit
          --train_file TRAIN_FILE
                                training data file (hdf5)
          --val_file VAL_FILE   validation data file (hdf5)
          --test_file TEST_FILE
                                test data file (hdf5)
          --max_length MAX_LENGTH
                                maximum length of training sentences. -1 means no max
                                length
          --w2v_file W2V_FILE   pretrained word vectors file (hdf5)
          --embedding_size EMBEDDING_SIZE
                                word embedding size
          --hidden_size HIDDEN_SIZE
                                hidden layer size
          --intra_sent_atten    whether to use intra sentence attention
          --epoch EPOCH         number of training epochs
          --gpus GPUS           number of gpus to train on. -1 for all gpus
          --val_interval VAL_INTERVAL
                                interval for checking the validation dataset
          --num_workers NUM_WORKERS
                                number of workers in the dataloader
          --optimizer {adam,adagrad}
          --lr LR               learning rate
          --max_grad_norm MAX_GRAD_NORM
                                if the norm of the gradient vector exceeds this
                                renormalize it to have the norm equal to the
                                max_grad_norm
          --param_init PARAM_INIT
                                parameter initialization gaussian
          --weight_decay WEIGHT_DECAY
                                l2 regularization
          --log_dir LOG_DIR     log file directory
          --model_path MODEL_PATH
                                path of the model
          --use_wandb           whether to use wandb for logging
        ```
---
